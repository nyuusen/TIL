# 改訂新版プロになるためのWeb技術入門

## 参考にしたもの

https://www.youtube.com/live/5WZDiKol8k4?si=snrgHWB_KgGl3jTe

## 3章 WWWの基本要素とその発展

- データ交換にHTMLは適さない(プログラムで取り扱うのが困難)
- そんな背景があり登場したのがXML
- どういうデータ構造なのかの決まりをスキーマという
- XMLではDOCTYPE宣言でdtdファイルでスキーマを指定できる
- スキーマレスのJSONやYAMLが流行ったが、結局JSONスキーマが流行ったり、流行は繰り返される
- XMLは人が扱うには複雑ではあるが、今でもExcelやe-taxなどコンピュータが読み取るデータとしては
多く利用される
- そこからXHTMLも開発されたが、制約が多かったりあまり歓迎されなかった(流行らなかった)

## 4章 HTTPクライアントとHTTPサーバー

- CGI: リクエストパスに応じてプログラムを起動する、今ではあまり見かけないが、このあとの発展に重要
- 動的にHTMLを返す、テンプレートエンジンの登場
- 上記を礎に、ブラウザなのでインストール不要であり、自由度の高いUIを実現できるWebアプリケーションが発展していくことになる

## 5章 URLとHTTP

- URLからwww.が消えたのは、クラウドの台頭とサブドメインを気軽にはやせることができるようになり、昔のようにホストとWebサーバの結びつきが弱くなったため
- 開発時にウェルナウンポートを使用すると起動のたびにsudo権限が必要なため、8080のようなポートを使うことが多い
- ブラウザはURLパスに含まれる拡張子ではなく、MIMEタイプ(Content-Type)を見てどのように扱うかを決めている
- GETリクエストでリソース状態を変更できるような実装すべきじゃない理由は、検索クローラによるアクセスでリソースが意図せず変化してしまうことを避けるということも理由にある
- よく見るフォーム再送信の確認ダイアログはPOSTリクエストが二重に行われることによる、不利益を避けるため

## 6章 従来型のWebアプリケーション

- 従来型アプリケーションをSPAとの対比という意味で昨今はMPAと呼ぶことが多い（当時からMPAと呼んでいたわけではなかった）
- コンピュータの世界で、一連の処理の流れにある背景情報(ユーザ名とか)をコンテキストという
- HTTPはステートレスである
  - 一連の流れをセッションという
  - ブラウザでそのセッション（状態）を管理するのがCookieである
- Cookie
  - Cookieの送信条件は「同じサイトにアクセスした時にそのサイトのCookieを渡す」
    - 同じサイトかは、オリジン（ホスト/スキーム/ポート）で判断する
  - ただそれ以外にも、サーバーからSet-Cookieヘッダでレスポンスするときに以下のような属性(細かな送信条件)を設定することが可能
    - Expires/Max-Age
      - 有効期限を表す
      - どちらも設定されている場合はMax-Ageが優先される
      - どちらも設定されていない場合はセッションクッキーと呼ばれ、ブラウザが終了するまで有効となる
        - だが近年のいくつかのブラウザは起動時にタブを復元する機能があり、その際にクッキーも復元される
      - Domain
        - どのサーバに対してアクセスした時にクッキーを送信すべきかを設定する
          - つまり、サーバからクライアントへの送信条件の制御
        - example.comと指定した場合は、sub.example.comのようなサブドメインも含まれる
          - 未指定の場合は、上記のようなサブドメインはCookieは送信されない
        - 例えば親ドメインで発行したクッキーを、サブドメインにあるサーバに送ってほしい時とかに使用する
      - HttpOnly
        - JSからのアクセスを禁止する
      - 外部への漏洩を防ぐために推奨される設定
        - Domain: 未指定（発行したドメインだけに送信されるようにする）
        - Secure: HTTPS通信時のみ
        - HttpOnly: JSからアクセス禁止させる
        - SameSite: Lax or Strict
      - DomainとSameSiteの違い
        - Domainは送信先の制限、SameSiteは送信元の制限
        - Domain指定なし:
          - 同一ドメインしかCookieを送信しない
        - Domainにexample.comを設定:
          - sub.example.comにもCookieを送信する
        - SameSite指定なし(Lax):
          - 一部のクロスサイトリクエスト（GETメソッドのナビゲーションなど）でもCookieを送信する
        - SameSiteにStrictを設定:
          - 同一サイトのみCookieを送信する
          - malicious.comというサイトからexample.comへのアクセス時はCookieを送信しないといったイメージ
        - SameSiteにNoneを設定:
          - いかなるクロスサイトのリクエストでもCookieを送信する
        - まとめると、
          - Domainは、どこにCookieを送信するかの設定
          - SameSiteは、どこからCookieを送信するかの設定
- セッション
  - Cookieを用いてセッションを管理する
  - セッションの盗用(セッションハイジャック)を防ぐ方法として
    - 固定や推測されやすい値を使用しない
- サードパーティクッキーと個人情報保護
  - あるサイトAに訪問した時に広告画像をクリックした際に、このユーザー(ブラウザ)はこの広告をクリックしたよというCookieをセットする
  - 別のサイトBにアクセスし、広告画像を表示する際に、広告側にはこのユーザーが過去にクリックした広告情報をCookieから取得し、異なるサイト間でも同じような広告を表示させるという仕組み
- セッションとユーザー管理
  - セッションベースでのToDoアプリ例を見ると、セッション単位＝ブラウザ単位となるので、ユーザー情報を異なるブラウザで保持させることができない問題がある
  - ここで登場するのが「認証」という仕組みになる

## 7章 SPAへの進化

(この章の背景を補足)
- 第2次ブラウザ戦争を経てGoogleがV8というJSエンジンを開発し、JSがとても速く実行できるようになった
- その結果、こんな速いならサーバーサイドでも使えるんじゃね？となり、Nodeが登場

- 従来型の遷移が多く遅い・サーバからの通知ができない問題を解決したのがSPA
- RIA(Flash)が流行りきらなかった理由としてクローズドな技術だったことが挙げられる。以降は標準化された技術の積み重ねで進化してきている
- DOM
  - ブラウザはHTMLを読み込むと、内部でDOMというツリー状のデータ構造を構築する
  - このDOMがJavaScriptからHTMLを操作するための仕様
- イベントドリブンプログラミング
  - 一般的なプログラムは逐次処理で上から下に記述された順で処理されるが、GUIのようなユーザー操作が基点となる処理では、逐次処理の書き方が複雑になる
  - そこで生まれたのがイベントドリブンで、ユーザ操作などのイベントを起点とし、そのイベントが発生したときに実行すべき処理を、そのイベントと紐づけて記述する。これをイベントハンドラやイベントリスナと呼ぶ。
  - JSでは、addEventListnerで関数登録できる
- JavaScriptは、ブラウザ戦争やブラウザ間の互換性差異に課題があった背景があり、ECMAScriptという標準化団体が標準化を進めた
  - これにより、各ブラウザに内蔵されるJavaScriptエンジンは異なるものの、ECMAScriptに準拠しているため、互換性が保たれている
- 同期処理から非同期処理へ
  - 同期処理だと、処理中にブラウザでの操作がブロックされ、体験があまりよくない
- XMLHttpRequest
  - XMLとついているが、実際にはXMLと強い結びつきはない
  - 事実JSONでやり取りすることが多い
- XMLとJSON
  - XMLは構造化されたデータをテキストでやり取りできるが、パース処理が比較的重いのが欠点
  - XMLの代わりとなったのがJSONで、JSの文法をそのまま使ってデータ表現をしている（のでパースが楽）
- ReactやVue.jsの存在
  - DOM APIのみを使って、JSで画面を構築するのは結構大変なので、表題のようなFWを使用する
  - SPAでは画面表示内容を全てJSで構築する
- フラグメントによる状態変化の表現
  - SPAはURLパスが変わらないので、ブックマークやブラウザの戻る進むができない
  - フラグメントは、見出しごとにアンカーを定義して、フラグメントでそれを参照するみたいなやつ
  - URLのフラグメント部はサーバーへ送信されない（リクエストが発生しない）
  - JSからはlocationオブジェクトで参照可能
  - SPAではフラグメントを活用することで、状態ごとにURLパスを変えることができる（というよりは、フラグメントを元に表示状態を復元している）
- SPAの課題
  - 検索エンジンとの相性の悪さ
    - SPAはHTMLがほとんど空っぽなので（JSで構築するので）クローラが(JS実行前の何もない状態の)HTMLを解析しても適切にインデックスが構築できない
    - また、URLのフラグメント部はインデックス対象にならないので、フラグメント部に固有の商品ID的なものを埋め込んでも、商品固有のインデックスを構築できない
    - ただ、現在はJSでレンダリングされたページでもある程度読み取れるように、クローラが改善されている
  - 初期表示の遅さ
    - JSで必要な情報をAPIから収集したり、その上で画面を構築していくので、どうしても初期描画が遅くなる
- SPAの課題に対するアプローチ
  - HistoryAPIによる画面遷移
    - HTML5で登場したブラウザのAPI
    - アドレスバーに表示させるURLや履歴をJSから操作できる
    - SPAの非同期通信を活用した高速な画面切り替えという強みを残しつつ、URL変更に伴う擬似的な画面遷移を実現できるようになった
    - 実装的には、aタグリンククリック時のイベントリスナのコールバック関数の中で、`e.preventDefault();`を実行し、ブラウザが本来行うページ遷移機能を奪い、JSでHistoryAPIを実行する
    - もし仮にURLパスが含まれた状態でリロードされてもきちんとページが返るように、サーバ側ではindex.htmlを返すように実装する必要がある
    - HistoryAPIとフラグメント方式は一長一短
      - アプリケーション内の画面を検索エンジンにインデックスさせる必要がないなら、フラグメント方式でも良い（実際Gmailはフラグメント方式）
      - フラグメント方式は、サーバ側での実装考慮が不要であり、実現方法が簡単
      - HistoryAPI方式は、サーバ側での考慮必要で実装が複雑だが、検索エンジンとの相性が良い
  - サーバーサイドレンダリングへの回帰
    - HistoryAPIにより、SPAでもフラグメントに頼らず、画面とURLを一致させられるようになったものの、JSが実行されるまではページ内容がわからない＝検索エンジンがインデックスできないという問題がある
    - それに加えて、初期表示の遅さという2つの課題を解決するものとしてサーバーサイドレンダリング
    - これは初期表示に限って、HTMLをサーバー側で生成して返すというもの
    - ちなみに初期表示の遅さは色々な要因があり、JSのコードを1つにまとめて、変数名や関数名を短くするミニファイ等がある
- AltJS
  - JSはES2015まで停滞していた、サーバーサイドとは異なりブラウザで唯一動く言語の停滞はクライアントサイドの開発においては大きな影響がある
  - そんな中、JSにトランスパイル言語を開発することで、それらをAltJSと呼ぶ
  - 生き残ったのがTypeScriptとなる

## 8章 WebAPI

- SPAの普及により、URLがコンテンツを返すではなく、アプリケーション・プログラムの関数呼び出しの位置付けに変化
- インターネットを経由したAPIの呼び出し、すなわちWebAPIが普及していった
- COBRAとSOAPを経て、WebAPIの普及へ
- WebAPIは設計の自由度が高いので、1つの指針を示したのがRESTになる
- RESTをWebAPIに当てはめて具体化する（リソース指向アーキテクチャ）
  - さまざまな情報の集まりをリソースと表現する
  - RESTに基づいたAPIは、リソースに対する操作を提供するという考え方で、次の4つの特性を持つ
    - リソースの示し方：リソースを表すURLがあり、URLによってリソースにアクセスできる
      - /addとかではなく、/todoというリソースを指す場所
    - リソースのたどり方：あるリソースから別のリソースを辿ることができる
    - リソースの操作方法：リソースに対して何をしたいかはHTTPメソッドで表現する
    - リソース操作の手順：リソース操作に手順は不要で、一度のやり取りで求められる結果が得られる
      - ステートレスは、サーバーに状態を持たせてしまうとシステム規模に応じてサーバー台数を増やしにくくなってしまうため
- POSTとHTTPステータスの使い分け
  - POSTメソッドには、以下の2つの役割がある
    - 子リソースの新規追加（TODOリストへの追加等、フォルダの中に新規ファイルを1つ作成するイメージ）
    - 既存リソースへの追加（既にあるファイルに追記するイメージ）
  - 子リソースへの追加の場合は、201 CreatedでLocationヘッダに作成したリソースのURLを返すべきとされている
  - 一方、既存リソースへの追加の場合は、対象が既に存在しているため、200 OKを返すだけで良いとされている
- オーバーロードPOST
  - 本来使用したいメソッドが使えない場合、POSTリクエストのヘッダに`X-HTTP-Method-Override`として本来使用したいメソッド(DELETE等)を設定する手法もある
  - なお、`X-`で始まるヘッダは非標準のカスタムヘッダを意味する
- 再注目されるRPCスタイル
  - RESTに則ったAPIでは「オーバー/アンダーフェッチング」「N＋1問題」がある（つまり欲しい情報を手に入れるのに足りなかった理、逆に多すぎるというケースがあると理解）
  - そこで注目されたのが「GraphQL」である
    - QueryLanguageなので、SQLと立て付けは同じで、SQLはデータを対象とし、GraphQLはオブジェクトを対象とする
    - 問い合わせ対象となるデータ構造を「スキーマ」として問い合わせパターンを定義する
    - 問い合わせパターンを「クエリ」と表現し、パラメータ部分（どのデータが欲しいか等）はクライアント側で自由に決められる
    - /graphqlという1つのエンドポイントに、クエリ文字列をPOSTメソッドを送信する
      - Mutationというデータ更新の場合も同じ（クエリをURLに含めることができず、ボディにセットする必要があるためこの仕組みになっている）
    - 欲しいデータを指定できるので、「オーバー/アンダーフェッチング」「N＋1問題」も解決
  - GraphQLは、SOAPのようなRPCに近い仕組みになっている
    - SOAPは、RESTによって駆逐されてしまったが、結局APIだけでは全てのニーズを満たすことができなかった
    - どっちがダメではなく相互補完である

## 9章 サーバプッシュ技術

### サーバプッシュ技術の歴史

- 当初のWWWで新たな情報を取得するためには同じURLにアクセスしてコンテンツを取得する必要があった
  - これをPull型通信と呼ぶ
- これに対して、サーバからクライアントに情報を送信することをサーバプッシュと呼ぶ

### メタリフレッシュによる擬似サーバプッシュ

- 初期のWWWで使用されていたのがmeta refreshというHTMLの機能
- meta要素に従って、URLを再ロードさせる
- 簡単ではあるが、即時性が求められるシーンでは現実的な手段ではないし、ページリロードが走るのでUX的にも良くなかった

### 通信量の削減

- ブラウザキャッシュという仕組み
- サーバ側がコンテンツ提供時にキャッシュ有効期間をきめ、その期間内はブラウザ側で保持し、それを表示に使用することで通信量を削減する

### Ajaxによるポーリング

- setInterval関数で定期ポーリング
- 定期的にFetchを行い、変更が必要な情報だけをJSONで取得することで、ページ全体のHTMLを返すよりも通信量を削減できる

### Comet(ロングポーリング)

- これは、クライアントがリクエストを送り、サーバはしばらく待ってからレスポンスを返すことでプッシュ風に見せる手法（今や古典的）
- キャッシュやAjaxでは即時性を向上させるのが難しい
- HTTPレスポンスの返し方を工夫することで、HTTPの枠組みの中で即時性を向上させる技術「Comet」が登場した
- Cometとは？
  - 意図的にレスポンスを保留し、通知すべき情報が変化が発生して初めてレスポンスを返す
  - サーバは待ち時間が発生するがAjaxによる非同期通信であればユーザーに影響はない
  - クライアント側はレスポンスを受け取ったらすぐにリクエストを送り（以下ループ）
- Cometの問題点
  - 一度サーバから情報を送信すると再接続が必要なので、高頻度で更新されるような場合はネットワークやサーバに負荷がかかる
  - また、一定時間経過すると、ブラウザや途中経過のプロキシサーバによって、通信が切断される
  - 即時性の面でも、レスポンスを返し、再度接続されるまでの間は、すぐに通知ができない
- という問題点を抱えたが、既存の技術を使ってプッシュ技術が実現できるので、長く使われた

### Server-sent eventsによるプッシュ配信

- Cometは既存の技術を使用し、無理矢理実現している感があった
- そんな状況で、HTML5で関連のAPIの1つとして制定されたのがServer-sent events
- Cometによるロングポーリングを1歩進めたHTTPストリーミング
- ロングポーリングでは1度レスポンスを返すと再接続が必要という課題があったが、HTTPストリーミングではレスポンスを少しずつ返すことで、一度のリクエストに対してサーバからの通知を連続して行うことができる
- これはHTTP1.1で追加されたチャンク転送という機能に基づいている

### チャンク転送

- chunkとは塊のこと
- リクエストやレスポンスをいくつかの塊に分割して送信する機能
- チャンク転送でレスポンスを返すには、`Transfer-Encoding: chunked`というヘッダを返し、以下のような形式でデータの塊を少しづつ返す
  ```
    <chunk size(16進数)>
    <data>
    <blank line>
  ```
- データを全て送信したら、最後に0を送信して、転送終了の合図とする
- クライアントはすべてのデータを持つことなく、チャンク単位で受け取って処理ができるので、総量が分からないデータを送信するのに役立つ

### Server-sent events(SSE)

- SSEとは、サーバーからクライアントへの一方向リアルタイム通信を実現する技術で、クライアントはJavaScriptのEventSourceというAPIを使い、サーバーからのイベントストリームを受け取る
  - 普通のHTTPリクエストを送った後、サーバーが`Content-Type: text/event-stream`でストリームを送り続ける
  - あくまでMIMEタイプtextで、UTF-8エンコードされたテキストを配信するので、バイナリには向いてない（Base64エンコードしてテキストに変換して送ることは可能ではあるが、サイズ膨らむし非効率なので..）
- SSEの考え方はチャンク転送と同じだが、レスポンスの返し方が少し異なる
- `Content-Type: text/event-stream`というヘッダが、SSEによる応答を表す
  - ついでにConnection: keep-aliveも必要で、これはリクエストを受信したサーバがレスポンスを返した後も通信を持続させることをお願いするもの
- SSEでは、サーバから通知する情報をチャンクとして返すが、個々をイベントと呼ぶ
- イベントの連なりによって構成される応答全体を「イベント・ストリーム」と呼ぶ
- チャンク転送とは異なり、サーバーかクライアントのどちらかが通信を止めない限り、通信が続く
  - チャンク転送のような最後に0を送信して終わり合図みたいなものはないので、必要であれば終了を表す合図を決めても良い
- SSEの受信処理
  - チャンク転送と異なり、受信側であるブラウザは標準APIで受信を簡単に処理できる（EventSourceオブジェクト）
- SSEのメリット
  - 標準APIで扱えるので、Comet等に比べると手軽に利用できる
  - HTTPで通信するので、FWやプロキシなど通信経路上の機器の設定変更等が不要
- 注意点
  - 多くのブラウザではサーバ側の負荷をかけない目的で、同一ドメインのサーバーに対する同時接続は6つまで
- SSEのサーバー側の送信処理
  - レスポンス自体は簡単だが、内部処理は少し難しくなる...
  - サーバーがブラウザからリクエストを受信するとサーバーはイベントを送信し続ける必要があるが、その間にもサーバー側はToDoの取得や追加のリクエストを処理することができる
  - ToDoの追加や更新があった時に、それをサーバー内部でSSEの送信処理に通知する必要がある
    - これを実現するには「並行処理」という手法が必要で、この本で紹介されているサーバー実装はGoで書かれているため、ゴルーチンを起動し、チャネルでゴルーチン同士で情報伝達を行っている

### WebSocket

- CometもSSEも、クライアントからリクエストを送信する必要があるというHTTP本来の性質は変わっておらず、双方向通信ができないという点やオーバーヘッド（TCP接続処理やヘッダ）が大きいという点に改善の余地があった
- そんな中生まれたのが双方向・高頻度・大量の通信を実現するWebSocketという新たなプロトコル
  - Comet、SSEはあくまで既存のHTTPに則っていたのに対し、ほとんど別のプロトコルとして作られた(httpではなくws)
  - 最初はHTTPで通信を始め、プロトコルアップグレードという仕組みでWebSocketに切り替える
  - HTTPと比較すると、ヘッダ相当の情報のやり取りがなく、送受信に伴いオーバーヘッドがとても少ないのが特徴
    - HTTPは毎回封筒に入れてやり取り、WebSocketは電話を繋げっぱなしでやり取りみたいなイメージ
  - イメージは以下
    - ((SSE), HTTP) --- TCP --- IP
    - HTTP |→| WebSockets --- TCP --- IP
- WebSocketのハンドシェイク
  - 通信を始める時のハンドシェイクでプロトコルアップグレードという仕組みを使用する
  - これは簡単にいうと最初はHTTPで通信を開始し、ここから別の言葉で話しましょうと別のプロトコルに切り替えるもの
    - HTTPリクエストヘッダに`Connection: Upgrade`, `Upgrade: websocket`を指定することで、クライアントがWebSocket通信を希望することがわかる
  - WebSocket上でどのような通信をするかはアプリケーションに委ねられている
  - WebSocket上での通信の取り決めは「サブプロトコル」といい、クライアントが希望するサブプロトコルを`Sec-WebSocket-Protocol`ヘッダで通知できるようになっている
  - `Sec-WebSocket-Key`ヘッダは、クライアントとサーバーが互いにWebSocketに対応していることを確認するためのもので、クライアントからはnonceと呼ばれるランダムに生成した文字列をセットする
- サーバーからの応答
  - クライアントからのアップグレード要求に対し、101を返す（その要求を了承したことを示す）
  - クライアントから受け取った`Sec-WebSocket-Key`の値(nonce)を、規定通りに変換した値をレスポンスヘッダ`Sec-WebSocket-Accept`にセットすることで、サーバーがWebSocketに対応していることを確認する
    - 規定通りに変換: nonceをSHA-1ハッシュ化→Base64エンコード
    - 秘匿性はないが、WebSocketではないサーバーに対して、WebSocket通信を始めてしまうような事故を防ぐため
  - アップグレード後は常時接続となり、双方向にいつでもデータを送ることが可能となる（つまりサーバー側からのプッシュ配信も可能）
- プロトコルアップグレードの必要性
  - 一般的なサーバ側のセキュリティ設定として80番/443番ポートを防ぐことが多く、仮にWebSocketが別のポートを使用してしまうと、経路の途中で遮断されてしまう可能性があるため
- WebSocketの課題
  - SSEと同じようにTCPコネクションが接続しっぱなしになるので、ブラウザ側の同時接続数の制限を受けることもあるし、サーバー側もより多くのクライアントから接続を受け付ける必要がある
  - あくまでHTTPのフリをしているため、ハンドシェイク以降の通信はHTTPではないため、プロキシの処理内容によっては遮断されてしまうことがある
  - プロキシサーバーがある環境では、プロキシサーバーが仲介する通信はHTTP前提で作られていることが多いので、ハンドシェイク以降の通信ができなくなる可能性がある
  - 負荷分散の問題: 1つのサーバーと長時間のTCP接続を行うと、均等に負荷分散されず、特定のサーバーに偏る可能性がある
  - 認証の問題
    - 認証の（標準的な）仕組みがない
      - そのため、アプリケーション側で実装してあげる必要がある
      - 具体的には、WebSocketの通信ではHTTPヘッダーを送らないので、最初の通信でJWTを送るとかいう形で、アプリケーション側で独自実装する形となる
    - 同一オリジンポリシーの制約がない
      - このようになっている背景として、WebSocketは、WebアプリだけではなくネイティブアプリやIoT機器からの接続も想定されているため、Webブラウザ限定のSOPルールに縛られない仕様になっているみたい
      - 対策としては、サーバー側でOriginヘッダを検証する必要がある（Originヘッダはブラウザが自動的に付与するのでJSで改変することはできない）

### SSEとWebSocketの使い分け

※これは個人的に気になったことのメモ

- SSEが向いているケース
  - ニュース速報配信・株価リアルタイム表示・SNSの新着投稿ありの通知
  - つまり、サーバーからの一方向の通信でテキスト情報をやり取りしたいシーンで採用される
- WebSocketが向いているケース
  - チャット、ゲームなどの画像のストリーミングや音声のリアルタイム配信などが向いている
  - つまり、双方向・低レイテンシー・バイナリ対応が求められるシーンで採用される

## 付録

### 2進数と16進数

- 16進数を表記する場合は先頭に0x（ゼロエックス）をつける
- 16進数はHEXと表現することもある
- 16進数は4ビットを1桁で表現できるので、4ビット単位で区切って簡潔に表現できたり、2桁でバイト単位を表現できたりとできたりと2進数との相性が良い
- 16進数がよく使われるシーン
  - 文字コード
    - パーセントエンコーディングやUnicodeのコードポイント表記
  - カラーコード
  - IPv6アドレス
  - MACアドレス

### テキストとバイナリ

- コンピュータ内部での文字列は数字に割り当てられる
  - 例えばAであれば「65」
  - この対応は、ASCIIと呼ばれ、アメリカの国内規格として制定されたので、この対応表に含まれるのは大小アルファベットと数字といくつかの記号のみ
  - ASCII文字は95種類しかないので、1バイトの範囲に十分収まる
- テキストデータの符号化方式
  - コンピュータで文字を数値として表現することを「符号化方式」という
- 現在はUTF-8という符号化方式に統一されている
- HTTPではテキストデータでやり取りされており、人間が見て理解しやすいというメリットがある一方、通信効率が悪いというデメリットがある

### 文字コード

- 現代のコンピュータは、世界中の文字を扱えるようになっているため、文字コードの仕組みや体系は複雑になっている
- 日本国内では、日本語を取り扱う文字コードとして「Shift_JIS」が主流だったが、WWWの普及と世界中のコンピュータが情報を交換するようになり、単一の文字コード体系で世界中の文字を扱えるようになる必要が出てきたが、現在はUnicodeという体系に統一されている

### UTF-8とUnicodeについて

- Unicodeは、文字の番号表（この文字はU+XXXXだというやつ）
  - 世界中の文字に通し番号を振った辞書
  - 元々は各国独自で文字コードを作っていた（日本はShift_JIS）が、異なる国の文字が混ざったときに文字化けが発生するので、世界共通のUnicodeに統一された
- UTF-8は、符号化方式の1つでであり、その番号(U+XXXX)をバイト列に変換するルール
  - UTF＝Unicode Transformation Format Unicodeコードポイントを変換するフォーマット・ルールのこと
  - バイト列: 複数バイトが連なったもの（01100001 01100010 01100011（= "abc"））
  - 軽さや処理効率に応じてUTF-16や32があるが、Webでは軽さ重視のためUTF-8が採用されることが多い（多分ネットワーク経由でやり取りするときに少ないバイト列で表現できた方がデータ量が削減できるからという意味だと思う＋UTF-8は、複数バイトの順番をどう並べるかのビッグ/リトルエンディアンを気にしなくて良いという理由もある）
  - UTF-8では、英数(ASCII文字)は1バイト、日本語は3バイトとなっている
    - 文字境界の判別＝どこからどこまでを1文字とするかは、先頭のビットパターンで区別している（1バイト文字なら先頭ビットが`0`,2バイト文字なら先頭ビットが`110`...）
- 大前提として、
  - ネットワークは光のON/OFF、ストレージはセルがチャージされているかどうか、つまり0か1というビット単位でしか処理ができない（ハードウェアレイヤの都合）
  - じゃあビットでやり取りすれば良いじゃんという話だが、それでは処理効率が悪い
  - だからコンピュータ内部（CPUやメモリ等）では8ビットを1バイトを最小単位として処理するように設計されている
  - 1バイト＝8ビットである理由は、2の8乗＝256通りであり、英数字などを扱うのにちょうどよかったため
  - これに伴い、通信プロトコルとかでも1バイト単位で送信するようになっている
- UnicodeとUTF-8の役割
  - まずコンピュータにはUnicodeの文字と番号が紐づいた表があり、番号から文字を割り出して、実際に文字を表示している
  - じゃあその番号はどのように出されるかというと、ネットワークやストレージを介したデータはバイト列で通信・保存されているので、そのバイト列からデコード処理を行なって番号を割り出している
  - じゃあそのバイト列は？という部分は、UTF-8なりで符号化（エンコードしてバイト列に変換）を行われているものになる
    - HTTPでやりとりするときやHTMLでは、`Content-Type`でcharsetを指定することで、中身はこのルール(符号化方式)でエンコードしているよということを伝えている
    - それを見て、コンピュータ側でデコード処理をしている（デコード結果の番号からUnicodeの表を見て、人間がわかる文字を表示している）
    - ファイル周りではBOM(Byte Order Mark)をつけることで、このファイルは何のエンコードだよという
      - WindowsのExcelでファイルを開く場合、これがないとShift_JISと誤認されて、文字化けが発生する可能性がある