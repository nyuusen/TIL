# 再帰

## 再帰の基本

- ある大きな問題を「同じ形のより小さな問題」に分解して、最終的に解けるサイズになるまで繰り返す
- ベースケースという再帰が止まる条件が必要

## スタック

### スタック

- データを一時的に積み上げるための構造
- LIFO(Last in, First out)
  - 書類を積み重ねるイメージ

### コールスタック

- プログラムが関数を呼び出した時に、その呼び出し履歴を記録しておくスタックのこと
- プログラムで関数が呼び出されると、OSはコールスタックという領域にパラメータや実行終了後に戻るべきコード内の場所などの関数に関する情報を格納する
- A()-> B()と呼ばれた場合は、B()が終了したらA()に戻ってみたいな

#### 再帰とコールスタック

- 再帰関数を呼び出すと、自分自身が再びスタックに積まれていく状態
- そしてベースケースでreturnされると、スタックが一段ずつpopされていく

## フィボナッチ数列

- フィボナッチ数列を木構造で表すと木の末端側から処理が徐々に実行される
  - ＋演算子より、関数実行が優先されるため、このような処理となる
  - フィボナッチ数列の再帰関数のコールスタックはLIFO
- このような処理を行うアルゴリズムを深さ優先探索（DFS: Depth First Search）という
  - よく引き合いに出される幅優先探索（BFS: Breadth First Search）は、逆にツリーの根本から

## 計算量

- 計算量とは「ある問題を解くのにどれくらい手間を要したかを数値で表したもの」
  - 時間計算量：手順の回数
  - 空間計算量：必要とする記憶領域（メモリ）
  - 同じ問題を解くにも、アルゴリズムが複数存在し、アルゴリズムの効率によって所要時間が大きく変わる
- 入力のサイズnに対して、アルゴリズムがどれくらいの時間がかかるか予測を立てるために、CSではO記法と呼ばれる記法を頻繁に使う（これを使って、実際に計算しなくても大まかに計算時間を測ることができる）
- O(log n)は「増えるけどゆっくり増える」
  - 線形 O(n) よりはるかに少ない回数で処理可能ということを表現できる

## 末尾再帰

- シンプルな総和を返す再帰関数では、n個の関数が順にコールスタックにpushされる
  - ただしコンピュータには物理制限があるため、nが大きくなるとスタックの保持上限を超えてスタックオーバーフローと言う現象が発生する
  - スタックオーバーフローは、ベースケースが機能しておらず、再帰関数が無限に呼び出されたり、空間計算量が大きいことが原因として考えられる
- スタックオーバーフローを回避する方法として、再帰関数を末尾関数(tail recursion)と呼ばれる特殊な形に書き換えると言うのがある
  - 末尾再帰は、再帰処理がすべての処理の最後に行われるもの（ex: `return f();`)
    - `return x + f();`は、関数呼び出し毎に演算処理が実行されるため、再帰処理が関数最後の処理になっておらず、このタイプはヘッド再帰と呼ばれる
  - 末尾再帰は、呼び出し元の関数に戻る必要がないため、通常の再帰処理よりも速く実行される
- 末尾再帰を使うことによって、一部の言語では末尾呼び出し最適化（tail call optimization）という技術が適用されることがある
  - これは、再帰関数が呼び出された時に新しいスタックフレームを確保するのではなく、既存のスタックフレームの値が更新されるので、空間計算量を圧倒的に削減できるメリットがある
    - 関数は自信をpopし、次の関数を同じフレーム内にpushするので、空間計算量はO(1)になる
- 末尾再帰にすることで、自身をn回呼び出すだけでよくなるので、時間計算量がO(2のn乗)からO(n)に削減、空間計算量もO(n)からO(1)に削減できる

# スコープ

- ローカルスコープは、再帰で学習したコールスタックがポップされた時のことをイメージした通り、変数や引数は一時的なメモリに保存され、関数の処理が終了すると自動的に破棄される
- 一方、グローバルスコープに宣言された変数は、常にメモリ上に残る・占有する
  - バグの発生はもちろん、副作用の発生やメモリの無駄遣いにもつながってしまう
- すべての関数は個別のローカルスコープで実行される

## 副作用

- エラーには「文法エラー」と「論理エラー」がある
- 副作用とは「どこにある何かを、知らず知らずの内に変容させてしまっている」ことを意味する
- グローバル変数を使うと、意図しない副作用を引き起こすリスクが上がる
- ソフトウェア開発をする際は「副作用をなるべく引き起こさないこと」「必要最低限に留めること」を心がける

## 値渡しと参照渡し

- ある関数から、外部で定義された関数を呼び出すとスコープはどうなるか？
  - 呼び出された関数は、異なるローカルスコープを作成し、この時点で2つの親子関係を持たない別々のスコープが存在することになる
  - 関数の入力と出力によって、この並列な2つのスコープ間で値をやり取りできるようになる
- 仮引数と実引数
  - 仮引数(parameter):関数を定義するときに、外から値を受け取るための変数
  - 実引数(argument):関数を呼び出すときに実際に渡す値
- 仮引数にデータを渡す2つの方法
  - 値渡し
    - 実引数の値のコピーが、仮引数に渡される
    - メモリ上のセルに格納された変数のデータを、他のセルにコピーされる
    - 最近の全ての言語ではこの値渡しと呼ばれる方法で仮引数にデータを渡している
  - 参照渡し
    - 実引数のメモリアドレスが仮引数に渡される
    - つまり、2つの引数が同じメモリ上の場所を指すようになる（1つの引数に対して変更を加えると、もう一方にも変更が反映される）
  - 補足
    - Goは全て値渡しだが、スライス・マップ・ポインタは参照のように振る舞う（メモリアドレスのコピーを渡す＝アドレスを直接渡すわけではない）
      - なおスライスは要注意：[Go の Slice の落とし穴 #Go - Qiita](https://qiita.com/ktateish/items/1fdae8ac845da788a789)
    - JSはプリミティブは値渡し、オブジェクトは参照渡し
    - オブジェクトが参照渡しである言語が一定ある理由としては、メモリ効率（オブジェクトはサイズが大きい）・一貫したオブジェクト操作を提供する（どんなメソッドでも同じオブジェクトを共有できるOOPの整合性的な視点から）・そもそも関数内でオブジェクトを変更したケースが多い

## 静的、スタック、ヒープメモリ

- メモリ割り当ての3つの方法
  - 静的メモリ割り当て：
    - プログラムが実行される前であるコンパイル時に行われる
    - 割り当てサイズは固定で、プログラムの開始から終了まで変数は存続する
    - グローバル変数がこれ
  - スタックメモリ割り当て：
    - コンピュータが関数を呼び出す度に新しいメモリスペースを確保する
    - このスペースは、コールスタックと呼ばれる場所に一時的に保存され、関数で使用する変数のデータが格納される
    - スコープの終了とともに、変数とデータは削除され、スタック領域は削除される
    - 自動的に割り当てられるので、明示的に制御することはできないが、不要になったスペースは自動的に解放されるので、プログラムの実行を効率的に行うことができる
    - 関数呼び出し時にメモリが消費されるので永遠に関数を呼び続けることはできない。スタックの保持上限に達するとスタックオーバーフローが発生するという流れ。
    - 自動メモリ割り当て（automatic memory allocation）と呼ばれる
    - これは、関数呼び出し時など事前にサイズが確定されるものに使われる
      - 関数の引数に文字列やオブジェクトがある場合、サイズは事前に確定できないのでは？
        - Noである。それぞれの参照を渡すので、サイズは確定している
  - ヒープメモリ割り当て：
    - プログラムが明示的に割り当てることのできるメモリ領域
    - 動的メモリ割り当て （dynamic memory allocation） と呼ばれる
    - メモリを割り当てるには、newやmallocなどのキーワードをや関数が使用され、deleteやfreeなどの関数で解放する（メモリ有効期間はユーザーによって決定される）
      - Goの場合は`new(int)`
        - ただし、関数内でnewしたものを出力しない場合は、スタックに割り当てられる（この辺はGoは自動でやってくれる賢い）
        - あとは、関数でポインタを返したり、クロージャのキャプチャなどのケースでヒープに逃げる
      - ヒープのどこに置くかは、プログラム側ではなく、ランタイム(メモリアロケータ)が決める
    - スタックメモリのLIFOとは異なり、任意の順序でアクセスすることができる
      - スタックは一番上しかアクセスできない
    - ヒープメモリは隔離されている
      - なのでどのスコープからでも、アドレスを知っている限りアクセスできる
  - メモリリーク
    - ユーザーがヒープメモリの割り当てを削除し忘れて、そのメモリが2度と使われない状態を指す
    - メモリ性能低下の原因になるため注意が必要
  - ガベージコレクタ
    - CやC++のようにメモリを完全に制御できる言語ではメモリリークを回避する責任はユーザーにあるが、他のほとんどの言語ではインタプリタやコンパイラがその処理を行う
    - 使用しなくなったメモリをヒープメモリから自動的に削除する処理はガベージコレクタと呼ばれる
      - マークアンドスイープというアルゴリズムが使われる
    - ヒープメモリを自動で管理する言語では、手動で管理する機能がなく、全て自動で行われるので、データがスタックorヒープに保存されるだけを理解するだけでOK

# オブジェクト

- これまでは、原則として他のデータ型から構成されないプリミティブ型を中心に学習してきた
- プリミティブ型で表現するには限界があり、実世界には状態(state)と振る舞い(behavior)が存在する

## オブジェクトの状態

- クラス内のすべてのオブジェクトに対して共通する値はクラス変数として保存する
  - これは静的領域に保存される
- メンバ変数（インスタンス変数）として、オブジェクトが持つデータを状態を保存する
- thisやselfなどのキーワードで、現在のオブジェクトを参照するために

## 文字列とオブジェクト

- 文字列はプリミティブ型ではなくオブジェクトとして扱われる
- JavaScriptでは、プリミティブの文字列は自動的にStringオブジェクトに変換される
- 文字列は文字の配列として実装されていて、各文字や部分文字列に高速でアクセスすることを実現している
- 多くのプログラミング言語では、オブジェクト(String含む)を参照する変数は参照型と見なされる
  - 変数にはオブジェクトそのものが格納されているのではなく、メモリ位置への参照が格納される
  - 理由は、文字列オブジェクトが必要とするメモリの量を予測することは不可能であるため

### オブジェクト同士の比較

- オブジェクト同士の比較は参照に対して行われるので、同じ値を持っていてもfalseを返す

## オブジェクト参照

- インスタンスが作成されると、オブジェクトのデータを格納するためにヒープ領域がメモリ上に割り当てられる
  - なぜかヒープ領域か？
    - 実行時にどのくらいのメモリが必要か事前に確定できないため
    - スコープを超えて生存する可能性があるため
      - 関数外でも参照され続けることが多い
  - オブジェクトに対して変数を割り当てる際、ヒープメモリのアドレスが変数に

## 可変性vs不変性

- 可変オブジェクトは初期化された後に変更追加削除ができ、逆に不変オブジェクトは変更が不可
- JavaScriptの文字列は不変オブジェクトであり、letで宣言したとしても文字列のインデックスにアクセスして、一部の文字を書き換えるなんてことはできない
  - やり方としては、新しい値を同じ変数に再代入する（let宣言なのでできる）か、一度文字列を配列に変換してから再結合する

# リスト

- リストのデータ構造には「配列」と「連結リスト」がある
  - 効率性の観点から配列を使用することが多い
- リストというデータ構造がメモリ内でどう動作するか？どのように計算するか？を学習する

## 配列

- 連続したメモリにそれぞれの要素を格納するデータ構造
  - 配列先頭のアドレス＋各要素のバイト数×インデックスで、要素の位置を即座に特定できる
  - CPU的には、加算乗算だけなので非常に高速
  - 時間計算量は、O(1)となる
  - インデックスアクセスが高速である一方、途中の変更に弱い
  - ここで疑問に思ったのが、文字列を格納する配列の場合、各要素の文字列のサイズを事前に計算することは難しいので、インデックス計算は遅くなるのでは？
    - A.配列自体には、文字列そのものではなく、文字列のポインタが格納されているため、O(1)＝高速で計算することが可能
    -
- 一方連結リストは、各要素はメモリ上にバラバラに存在する
  - ノードは、次ノードへのポインタを持つ
  - 時間計算量は、O(n)となる
  - アクセスは遅いが、挿入や削除が速い
- 配列の注意点
  - 配列内では原則一種類のデータ型しか使用できない。JSやPHPなどは、混在したデータ型も格納できるが

## 動的配列

- これまで見てきたのは固定朝配列で、配列宣言後にサイズを拡大縮小することはできない
- 必要に応じて自動的に大きくしたり小さくしたりできる配列が「動的配列」
- 動的配列は、新しい要素を追加するときに既存のメモリスペースが不足している場合、自動的により大きなメモリブロックを確保し、そこに既存の要素をコピーする
- 要素が削除されるときは、物理的には削除されず、代わりに配列の論理サイズがデクリメントされ、要素が削除された場所は空きスペースとなる
  - この空きスペースは、新たに要素が追加される際に再利用される

### 動的配列の仕組み

- 「論理サイズ(logical size:要素数)」と「容量(capacity:最大要素数)」を常にメモリ内に把握
- 論理サイズと容量がイコールになった状態でさらに要素数が追加されると、論理サイズ*成長係数(1.5や2が多い)
- 動的配列は便利である一方、メモリが無駄になることもあるので、メモリが重要な状況においては使用を考える必要がある

### 計算量

- 先頭への挿入
  - O(n)
  - n個の要素を移動させる必要があるため
- 途中への挿入
  - O(n)
  - 先頭への挿入を、配列の真ん中まで行うので、0.5n個の要素を移動させる必要がある
- 途中の削除
  - 途中への挿入と同様O(n)
- 末端への挿入
  - 配列の容量が最大に達している場合
    - 元々の要素数n個と追加した1個でO(n)
  - 配列に余裕がある場合（ほとんどこれ）
    - O(1)
- 末端の削除
  - 常にO(1)で済む

## 二次元配列

### 二次元動的配列の参照

- 静的な二次元配列のようにメモリ上に連続的に格納されず、実際のデータが格納されるメモリアドレスの参照が格納される
- 主配列には副配列の参照アドレスが格納される

## 探索

- コンピュータは人間と違って融通が効かないので先頭から順繰り要素を探索する
- なので、配列探索の時間計算量はO(n)
- ただし、あらかじめインデックスが特定できている場合はO(1)となる
  - このロジックはデータベースのインデックスにも使われる

## 連想配列

- インデックス以外の文字列を使って、効率よくデータを取得するのが連想配列
- 言語によっては、マップや辞書と呼ばれることもある

### 連想配列の実装

- 多くの言語では、文字列は文字の並びで、内部では数字の並びとして表現される
  - これらの数値はASCIIやUnicodeなどの文字エンコーディング仕様における特定の文字に対応している
- 検索効率性を上げるため、キーは通常ハッシュ関数でハッシュ化されている
  - ハッシュ値は通常数字で、キーに含まれる文字のコードポイントに基づき計算される

## キャッシュ

### エラトステネスの篩（ふるい）

- n未満の全ての素数を少ない計算量で求める古典的なアルゴリズム

### 動的計画法

- フィボナッチ数列の末尾再帰は、既に行った計算を何度も繰り返すので、計算量はO(2のn乗)となる
  - これに対し、計算結果をテーブルに保存すること（キャッシュを活用）することで、時間計算量はO(n)となる
  - このようにキャッシュを使うことによって、再帰的な問題を解決する方法を「動的計画法」と呼ぶ
    - 実装方法としては「メモ化」と「タビュレーション」がある
- タビュレーション
  - ボトムアップ方式で、計算結果をリストに格納する
- メモ化
  - ボトムダウン方式

### ハッシュマップキャッシング

- ハッシュマップの計算量はO(1)

## トランプカードの設計

- シャッフルに使うアルゴリズムin-placeアルゴリズム
  - 追加のメモリスペースを必要とせず、既存のデータ構造内で操作を行うアルゴリズム
  - 配列内のランダムなインデックスの要素を1つ取得し、それを一時変数に保存→

## ソート

### 選択ソート

- 配列の未ソート部分から最小の要素を選び、現在の位置の要素と交換することを繰り返すシンプルなソートアルゴリズム
- 要素をソートするために追加のスペースを必要とせずin-placeアルゴリズムと呼ばれる

### 挿入ソート

- 1つずつ順番に要素を取り出し、その要素をソート済みの部分の適切な場所に挿入していく
  - 最初に、最初の要素をソート済みとみなす
  - 残りの配列を処理し、各要素をソート済み部分の適切な場所に挿入する
  - 挿入は、ソート済み部分の要素を右にずらし、新しい要素のためにスペースを作る
- 5,4,3,2,1みたいなケースが最悪で計算量O(nの2乗)となる
  - 全てのデータがソートされていると仮定されるランキングシステムによく使われる

## 分割統治法

- 大きな問題を、同じ形の小さな問題に分けて、それを解いて、最後にまとめる

### マージソート

- 分割統治法を使った代表的なソートアルゴリズム
- 「配列を半分に分けて → 小さくした配列をソートして → 最後にマージする」という流れ
- 時間計算量は、O(n log n)（n × log n）

### クイックソート

- 分割統治法を使った高速なソートアルゴリズム
- 基準を1つ選んで、それより小さい値を左のグループ・大きい値を右のグループに分ける
  - これをそれぞれのグループで繰り返し行う
  - 最終的に全部整列する
- 多くのプログラミング言語の標準ソートは、クイックソートもしくはその亜種にしているケースが多い
- 平均速度は速いが、基準選びが最悪だとO(nの2乗)となる
